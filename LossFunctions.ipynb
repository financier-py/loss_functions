{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Функции потерь в машинном обучении\n",
    "\n",
    "## Предпосылки\n",
    "\n",
    "Перед нами стоит задача регрессии, у нас есть:\n",
    "\n",
    "- Объекты\n",
    "- Целевая переменная $y$\n",
    "\n",
    "Наша цель — построить модель $\\alpha(x)$, которая по описанию объекта $x$ будет выдавать прогноз $\\alpha(x)$, максимально близкий к истинному значению $y$.\n",
    "\n",
    "Для построения такой модели у нас есть **обучающая выборка** - набор объектов, для которых мы уже знаем правильные ответы. Обозначать мы ее будем так: $X^l = \\{(x_1, y_1), (x_2, y_2), \\dotsc, (x_l, y_l)\\} = \\{(x_i, y_i)\\}_{i=1}^l$, где $l$ — количество объектов в обучающей выборке и $x_i \\in \\mathbb{R}^n$.\n",
    "\n",
    "В нашем случае модель $\\alpha(x)$ будет линейной функцией от признаков объекта $x$:\n",
    "\n",
    "$$\\alpha(x) = w_0 + w_1 x_1 + w_2 x_2 + \\dotsc + w_n x_n,$$\n",
    "\n",
    "где $w_0, w_1, \\dotsc, w_n$ — параметры модели **(веса)**, которые нам предстоит найти.\n",
    "\n",
    "Также модель можно записать в векторном виде:\n",
    "\n",
    "- Вектор весов: $w = (w_1, \\dotsc, w_n)^T$\n",
    "- Скалярное произведение: $\\langle w, x \\rangle = w_1 x_1 + w_2 x_2 + \\dotsc + w_n x_n$\n",
    "\n",
    "Тогда:\n",
    "\n",
    "$$\\alpha(x) = w_0 + \\langle w, x \\rangle$$\n",
    "\n",
    "Можно избавиться от свободного члена $w_0$, добавив в вектор признаков фиктивный признак $x_0 = 1$. Тогда вектор признаков будет иметь вид: $x = (1, x_1, x_2, \\dotsc, x_n)^T$. Вектор весов: $w = (w_0, w_1, \\dotsc, w_n)^T$. В этом случае модель можно записать так:\n",
    "\n",
    "$$\\alpha(x) = \\langle w, x \\rangle$$\n",
    "\n",
    "Прекрасно! Теперь мы хотим найти такие веса $w$, чтобы модель $\\alpha(x)$ наилучшим образом приближала целевую переменную $y$ на обучающей выборке $X^l$. Но как нам это сделать? Для этого нам нужна **функция потерь**, которая будет измерять качество нашей модели."
   ],
   "id": "99b6a0dc5c57611a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Mean Squared Error (MSE)\n",
    "\n",
    "Для одного объекта она определяется как:\n",
    "\n",
    "$$L(y, a) = (a - y)^2,$$\n",
    "\n",
    "где $a$ - предсказание модели, $y$ - истинное значение целевой переменной. Для всей выборки функция потерь будет выглядеть так:\n",
    "\n",
    "$$Q(w) = MSE(\\alpha, X) = \\frac{1}{l} \\sum_{i = 1}^l (\\alpha(x_i) - y_i)^2$$\n",
    "\n",
    "Отсюда же мы понимаем, что $Q(w) \\geq 0$\n",
    "\n",
    "Поскольку $\\alpha(x_i) = \\langle w, x_i \\rangle$, мы можем подставить это выражение:\n",
    "\n",
    "$$Q(w) = \\frac{1}{l} \\sum_{i = 1}^l (\\langle w, x_i \\rangle - y_i)^2$$\n",
    "\n",
    "По итогу мы получили задачу оптимизации:\n",
    "\n",
    "$$w^* = \\arg \\min_{w} Q(w) = \\arg \\min_{w} \\frac{1}{l} \\sum_{i = 1}^l (\\langle w, x_i \\rangle - y_i)^2$$\n",
    "\n",
    "Поэтому возьмем производную!\n",
    "\n",
    "$$\\frac{\\partial Q}{\\partial w_j} = \\frac{\\partial}{\\partial w_j} \\left[ \\frac{1}{l} \\sum_{i = 1}^l (\\langle w, x_i \\rangle - y_i)^2 \\right] = \\frac{1}{l} \\sum_{i = 1}^l \\frac{\\partial}{\\partial w_j}(\\langle w, x_i \\rangle - y_i)^2$$\n",
    "\n",
    "Понимаем, что:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_j}(\\langle w, x_i \\rangle - y_i)^2 = 2(\\langle w, x_i \\rangle - y_i) \\cdot \\frac{\\partial}{\\partial w_j}(\\langle w, x_i \\rangle - y_i) = 2(\\langle w, x_i \\rangle - y_i) x_{ij}$$\n",
    "\n",
    "Поэтому:\n",
    "\n",
    "$$\\frac{\\partial Q}{\\partial w_j} = \\frac{2}{l} \\sum_{i = 1}^l (\\langle w, x_i \\rangle - y_i) x_{ij}$$\n",
    "\n",
    "Отсюда уже можем получить сам градиент:\n",
    "\n",
    "$$\\nabla Q = \\left( \\frac{\\partial Q}{\\partial w_0}, \\frac{\\partial Q}{\\partial w_1}, \\dotsc, \\frac{\\partial Q}{\\partial w_n} \\right)^T = \\frac{2}{l} \\sum_{i = 1}^l (\\langle w, x_i \\rangle - y_i) x_i$$\n",
    "\n",
    "И его можем записать в матричном виде:\n",
    "\n",
    "$$\\nabla Q = \\frac{2}{l} X^T (X w - y)$$\n",
    "\n",
    "Тк $X w = (\\alpha(x_1), \\dotsc, \\alpha(x_l))$, а умножая на $X^T$ мы по сути умножаем каждую ошибку на соответствующий вектор признаков $x_i$.\n",
    "\n",
    "Теперь вычислим Гессиан:\n",
    "\n",
    "$$\\frac{\\partial^2 Q}{\\partial w_k \\partial w_j} = \\frac{\\partial}{\\partial w_k} \\frac{2}{l} \\sum_{i = 1}^l (\\langle w, x_i \\rangle - y_i) x_{ij} = \\frac{2}{l} \\sum_{i = 1}^l \\frac{\\partial}{\\partial w_k} (\\langle w, x_i \\rangle - y_i) x_{ij} = \\frac{2}{l} \\sum_{i = 1}^l x_{ij} \\cdot x_{ik}$$\n",
    "\n",
    "Или в матричном виде:\n",
    "\n",
    "$$H = \\nabla^2 Q = \\frac{2}{l} X^T X$$\n",
    "\n",
    "Заметим, что матрица $X^T X$ является матрицей Грама, а значит она положительно полуопределена. Следовательно, Гессиан $H$ тоже положительно полуопределен, а значит функция потерь $Q(w)$ — выпуклая функция. Это значит, что любой найденный нами локальный минимум будет также и глобальным минимумом $:)$\n",
    "\n",
    "Прекрасно! Теперь мы знаем, что минимум существует. Но как его найти? Попробуем аналитически.\n",
    "\n",
    "Приравниваем градиент к нулю:\n",
    "\n",
    "$$\\nabla Q = \\frac{2}{l} X^T (X w - y) = 0$$\n",
    "\n",
    "$$X^T (X w - y) = 0$$\n",
    "\n",
    "$$X^T X w = X^T y$$\n",
    "\n",
    "Если матрица $X^T X$ обратима, то мы можем найти решение:\n",
    "\n",
    "$$w^* = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "Вообще матрица $(X^T X)^{-1} X^T$ называется псевдообратной. Она находит проекцию вектора $y$ на линейное пространство, натянутое на столбцы матрицы $X$. То есть мы ищем такую линейную комбинацию столбцов $X$, которая будет максимально близка к вектору $y$.\n",
    "\n",
    "Какие есть недостатки у аналитического решения?\n",
    "- Вычисление обратной матрицы имеет сложность $O(n^3)$\n",
    "- Матрица $X^T X$ может быть необратима _(например, когда признаки линейно зависимы)_\n",
    "- При больших данных может быть сложно хранить всю матрицу в памяти, тк занимает $O(n^2)$\n",
    "\n",
    "Поэтому сейчас мы перейдем к градиентному спуску $:)$\n",
    "\n",
    "Вспомним, что градиент - вектор частных производных, который показывает направление наискорейшего возрастания функции. Следовательно, чтобы найти минимум функции, нужно двигаться в направлении, противоположном градиенту! Этим и будем мы пользоваться.\n",
    "\n",
    "Алгоритм градиентного спуска:\n",
    "\n",
    "Начинаем с какой-то начальной точки $w^{(0)}$ _(обычно выбирают нулевой вектор или случайный вектор)_. Затем на каждой итерации $t$ делаем шаг в направлении антиградиента:\n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\eta \\nabla Q(w^{(t)})$$\n",
    "\n",
    "где:\n",
    "- $\\eta$ — это **шаг обучения** (learning rate), который определяет, насколько большим будет наш шаг\n",
    "- $\\nabla Q(w^{(t)})$ — градиент функции потерь в точке $w^{(t)}$\n",
    "- $w^{(t)}$ — вектор весов на итерации $t$\n",
    "\n",
    "Теперь подставим наш градиент, который мы вычислили ранее:\n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\eta \\cdot \\frac{2}{l} X^T (X w^{(t)} - y)$$\n",
    "\n",
    "Однако, теперь есть вопрос, когда останавливать алгоритм? Вот несколько распространенных критериев остановки:\n",
    "- По числу итераций\n",
    "- По изменению весов: если веса почти не изменяются: $||w^{(t+1)} - w^{(t)}|| < \\epsilon$\n",
    "- По изменению функции потерь: если значение функции потерь почти не изменяется: $|Q(w^{(t+1)}) - Q(w^{(t)})| < \\epsilon$\n",
    "\n",
    "Что же, перейдем к реализации!"
   ],
   "id": "81f4e3d234f5f9b4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
