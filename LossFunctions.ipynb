{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Функции потерь в машинном обучении\n",
    "\n",
    "## Предпосылки\n",
    "\n",
    "Перед нами стоит задача регрессии, у нас есть:\n",
    "\n",
    "- Объекты\n",
    "- Целевая переменная $y$\n",
    "\n",
    "Наша цель — построить модель $\\alpha(x)$, которая по описанию объекта $x$ будет выдавать прогноз $\\alpha(x)$, максимально близкий к истинному значению $y$.\n",
    "\n",
    "Для построения такой модели у нас есть **обучающая выборка** - набор объектов, для которых мы уже знаем правильные ответы. Обозначать мы ее будем так: $X^l = \\{(x_1, y_1), (x_2, y_2), \\dotsc, (x_l, y_l)\\} = \\{(x_i, y_i)\\}_{i=1}^l$, где $l$ — количество объектов в обучающей выборке и $x_i \\in \\mathbb{R}^n$.\n",
    "\n",
    "В нашем случае модель $\\alpha(x)$ будет линейной функцией от признаков объекта $x$:\n",
    "\n",
    "$$\\alpha(x) = w_0 + w_1 x_1 + w_2 x_2 + \\dotsc + w_n x_n,$$\n",
    "\n",
    "где $w_0, w_1, \\dotsc, w_n$ — параметры модели **(веса)**, которые нам предстоит найти.\n",
    "\n",
    "Также модель можно записать в векторном виде:\n",
    "\n",
    "- Вектор весов: $w = (w_1, \\dotsc, w_n)^T$\n",
    "- Скалярное произведение: $\\langle w, x \\rangle = w_1 x_1 + w_2 x_2 + \\dotsc + w_n x_n$\n",
    "\n",
    "Тогда:\n",
    "\n",
    "$$\\alpha(x) = w_0 + \\langle w, x \\rangle$$\n",
    "\n",
    "Можно избавиться от свободного члена $w_0$, добавив в вектор признаков фиктивный признак $x_0 = 1$. Тогда вектор признаков будет иметь вид: $x = (1, x_1, x_2, \\dotsc, x_n)^T$. Вектор весов: $w = (w_0, w_1, \\dotsc, w_n)^T$. В этом случае модель можно записать так:\n",
    "\n",
    "$$\\alpha(x) = \\langle w, x \\rangle$$\n",
    "\n",
    "Прекрасно! Теперь мы хотим найти такие веса $w$, чтобы модель $\\alpha(x)$ наилучшим образом приближала целевую переменную $y$ на обучающей выборке $X^l$. Но как нам это сделать? Для этого нам нужна **функция потерь**, которая будет измерять качество нашей модели."
   ],
   "id": "99b6a0dc5c57611a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Mean Squared Error (MSE)\n",
    "\n",
    "Для одного объекта она определяется как:\n",
    "\n",
    "$$L(y, a) = (a - y)^2,$$\n",
    "\n",
    "где $a$ - предсказание модели, $y$ - истинное значение целевой переменной. Для всей выборки функция потерь будет выглядеть так:\n",
    "\n",
    "$$Q(w) = MSE(\\alpha, X) = \\frac{1}{l} \\sum_{i = 1}^l (\\alpha(x_i) - y_i)^2$$\n",
    "\n",
    "Отсюда же мы понимаем, что $Q(w) \\geq 0$\n",
    "\n",
    "Поскольку $\\alpha(x_i) = \\langle w, x_i \\rangle$, мы можем подставить это выражение:\n",
    "\n",
    "$$Q(w) = \\frac{1}{l} \\sum_{i = 1}^l (\\langle w, x_i \\rangle - y_i)^2$$\n",
    "\n",
    "По итогу мы получили задачу оптимизации:\n",
    "\n",
    "$$w^* = \\arg \\min_{w} Q(w) = \\arg \\min_{w} \\frac{1}{l} \\sum_{i = 1}^l (\\langle w, x_i \\rangle - y_i)^2$$\n",
    "\n",
    "Поэтому возьмем производную!\n",
    "\n",
    "$$\\frac{\\partial Q}{\\partial w_j} = \\frac{\\partial}{\\partial w_j} \\left[ \\frac{1}{l} \\sum_{i = 1}^l (\\langle w, x_i \\rangle - y_i)^2 \\right] = \\frac{1}{l} \\sum_{i = 1}^l \\frac{\\partial}{\\partial w_j}(\\langle w, x_i \\rangle - y_i)^2$$\n",
    "\n",
    "Понимаем, что:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_j}(\\langle w, x_i \\rangle - y_i)^2 = 2(\\langle w, x_i \\rangle - y_i) \\cdot \\frac{\\partial}{\\partial w_j}(\\langle w, x_i \\rangle - y_i) = 2(\\langle w, x_i \\rangle - y_i) x_{ij}$$\n",
    "\n",
    "Поэтому:\n",
    "\n",
    "$$\\frac{\\partial Q}{\\partial w_j} = \\frac{2}{l} \\sum_{i = 1}^l (\\langle w, x_i \\rangle - y_i) x_{ij}$$\n",
    "\n",
    "Отсюда уже можем получить сам градиент:\n",
    "\n",
    "$$\\nabla Q = \\left( \\frac{\\partial Q}{\\partial w_0}, \\frac{\\partial Q}{\\partial w_1}, \\dotsc, \\frac{\\partial Q}{\\partial w_n} \\right)^T = \\frac{2}{l} \\sum_{i = 1}^l (\\langle w, x_i \\rangle - y_i) x_i$$\n",
    "\n",
    "И его можем записать в матричном виде:\n",
    "\n",
    "$$\\nabla Q = \\frac{2}{l} X^T (X w - y)$$\n",
    "\n",
    "Тк $X w = (\\alpha(x_1), \\dotsc, \\alpha(x_l))$, а умножая на $X^T$ мы по сути умножаем каждую ошибку на соответствующий вектор признаков $x_i$.\n",
    "\n",
    "Теперь вычислим Гессиан:\n",
    "\n",
    "$$\\frac{\\partial^2 Q}{\\partial w_k \\partial w_j} = \\frac{\\partial}{\\partial w_k} \\frac{2}{l} \\sum_{i = 1}^l (\\langle w, x_i \\rangle - y_i) x_{ij} = \\frac{2}{l} \\sum_{i = 1}^l \\frac{\\partial}{\\partial w_k} (\\langle w, x_i \\rangle - y_i) x_{ij} = \\frac{2}{l} \\sum_{i = 1}^l x_{ij} \\cdot x_{ik}$$\n",
    "\n",
    "Или в матричном виде:\n",
    "\n",
    "$$H = \\nabla^2 Q = \\frac{2}{l} X^T X$$\n",
    "\n",
    "Заметим, что матрица $X^T X$ является матрицей Грама, а значит она положительно полуопределена. Следовательно, Гессиан $H$ тоже положительно полуопределен, а значит функция потерь $Q(w)$ — выпуклая функция. Это значит, что любой найденный нами локальный минимум будет также и глобальным минимумом $:)$\n",
    "\n",
    "Прекрасно! Теперь мы знаем, что минимум существует. Но как его найти? Попробуем аналитически.\n",
    "\n",
    "Приравниваем градиент к нулю:\n",
    "\n",
    "$$\\nabla Q = \\frac{2}{l} X^T (X w - y) = 0$$\n",
    "\n",
    "$$X^T (X w - y) = 0$$\n",
    "\n",
    "$$X^T X w = X^T y$$\n",
    "\n",
    "Если матрица $X^T X$ обратима, то мы можем найти решение:\n",
    "\n",
    "$$w^* = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "Вообще матрица $(X^T X)^{-1} X^T$ называется псевдообратной. Она находит проекцию вектора $y$ на линейное пространство, натянутое на столбцы матрицы $X$. То есть мы ищем такую линейную комбинацию столбцов $X$, которая будет максимально близка к вектору $y$.\n",
    "\n",
    "Перейдем к реализации аналитического решения!"
   ],
   "id": "81f4e3d234f5f9b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Реализация аналитического решения MSE\n",
    "# TODO"
   ],
   "id": "6f4bba2f56dcebff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Но какие есть недостатки у аналитического решения?\n",
    "- Вычисление обратной матрицы имеет сложность $O(n^3)$\n",
    "- Матрица $X^T X$ может быть необратима _(например, когда признаки линейно зависимы)_\n",
    "- При больших данных может быть сложно хранить всю матрицу в памяти, тк занимает $O(n^2)$\n",
    "\n",
    "Поэтому сейчас мы перейдем к *градиентному спуску* $:)$\n",
    "\n",
    "Вспомним, что градиент - вектор частных производных, который показывает направление наискорейшего возрастания функции. Следовательно, чтобы найти минимум функции, нужно двигаться в направлении, противоположном градиенту! Этим и будем мы пользоваться.\n",
    "\n",
    "Алгоритм градиентного спуска:\n",
    "\n",
    "Начинаем с какой-то начальной точки $w^{(0)}$ _(обычно выбирают нулевой вектор или случайный вектор)_. Затем на каждой итерации $t$ делаем шаг в направлении антиградиента:\n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\eta \\nabla Q(w^{(t)})$$\n",
    "\n",
    "где:\n",
    "- $\\eta$ — это **шаг обучения** (learning rate), который определяет, насколько большим будет наш шаг\n",
    "- $\\nabla Q(w^{(t)})$ — градиент функции потерь в точке $w^{(t)}$\n",
    "- $w^{(t)}$ — вектор весов на итерации $t$\n",
    "\n",
    "Теперь подставим наш градиент, который мы вычислили ранее:\n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\eta \\cdot \\frac{2}{l} X^T (X w^{(t)} - y)$$\n",
    "\n",
    "Однако, теперь есть вопрос, когда останавливать алгоритм? Вот несколько распространенных критериев остановки:\n",
    "- По числу итераций\n",
    "- По изменению весов: если веса почти не изменяются: $||w^{(t+1)} - w^{(t)}|| < \\epsilon$\n",
    "- По изменению функции потерь: если значение функции потерь почти не изменяется: $|Q(w^{(t+1)}) - Q(w^{(t)})| < \\epsilon$\n",
    "\n",
    "Перейдем к реализации град. спуска!"
   ],
   "id": "4a4dfca96709dd92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Реализация град. спуска для MSE\n",
    "# TODO"
   ],
   "id": "9e9d4cf0a386038b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Однако теперь взглянем на ф-лу обновления весов:\n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\eta \\cdot \\frac{2}{l} X^T (X w^{(t)} - y)$$\n",
    "\n",
    "Тут градиент вычисляется, как сумма по всем объектам выборки:\n",
    "\n",
    "$$\\frac{2}{l} \\sum_{i = 1}^l (\\langle w, x_i \\rangle - y_i) x_i$$\n",
    "\n",
    "Это значит, что на каждой итерации нам нужно:\n",
    "\n",
    "- Пройти по всем $l$ объектам\n",
    "- Вычислить ошибка для каждого объекта\n",
    "- Просуммировать вклады всех объектов\n",
    "\n",
    "Поэтому вычислительная сложность для каждой итерации $O(nl)$, где $n$ - число признаков, $l$ - число объектов\n",
    "\n",
    "Поэтому, например, если объектов у нас будет миллион, то на каждой итерации нам придется их все обрабатывать, что может быть медленно!\n",
    "\n",
    "Как же решить данную проблему? Для этого будем использовать **стохастический градиент**!\n",
    "\n",
    "Если так посмотреть, то градиент в нашем случае - это среднее по всем объектам. А среднее можно приблизить взяв один случайный объект или например небольшую группу объектов!\n",
    "\n",
    "То есть для определения направления мы будем брать только один объект $i$:\n",
    "\n",
    "$$\\nabla Q_i(w) = 2 (\\langle w, x_i \\rangle - y_i) x_i$$\n",
    "\n",
    "Но тогда может возникнуть вопрос, почему это вообще будет работать? Ибо один объект может дать совершенно неправильное направление!\n",
    "\n",
    "Для этого мы воспользуемся мат. ожиданием, оно покажет, что в среднем мы движемся правильно $:)$\n",
    "\n",
    "В нашем случае:\n",
    "\n",
    "- Эксперимент: случайно выбрать объект $i$ и вычислить $\\nabla Q_i(w)$\n",
    "- Результат эксперимента: $\\nabla Q_i(w)$\n",
    "- Мат. ожид.: какой вектор мы получим в среднем, если повторять эксперимент бесконечно много раз?\n",
    "\n",
    "$$\\mathbb{E}[\\nabla Q_i(w)] = \\nabla Q(w)$$\n",
    "\n",
    "Введем случайную величину $I$, которая будет принимать значения $\\{1, 2, \\dotsc, l \\}$ - номера объектов в выборке\n",
    "\n",
    "Если мы выбираем объект равномерно случайно, то:\n",
    "\n",
    "$$P(I = i) = \\frac{1}{l} \\ \\ \\forall i \\in \\{1, 2, \\dotsc, l \\}$$\n",
    "\n",
    "Теперь можем определить случайную величину $g_I(w)$, где $I$ - случайно выбранный номер объекта:\n",
    "\n",
    "$$g_I(w) = 2 (\\langle w, x_I \\rangle - y_I) x_I$$\n",
    "\n",
    "То есть она будет показывать вклад объекта $I$ в градиент!\n",
    "\n",
    "Поэтому:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[g_I(w)] = \\sum_{i = 1}^l P(I = i) \\cdot g_i(w) = \\sum_{i = 1}^l \\frac{1}{l} \\cdot g_i(w) = \\sum_{i = 1}^l \\frac{1}{l} \\cdot 2 (\\langle w, x_i \\rangle - y_i) x_i = \\nabla Q(w)\n",
    "$$\n",
    "\n",
    "По итогу в стохастическом градиентном спуске мы делаем:\n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\eta g_{i_t}(w^{(t)})$$\n",
    "\n",
    "Перейдем к реализации GSE!"
   ],
   "id": "4cfb1c24c56f27ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# реализация GSE\n",
    "# TODO"
   ],
   "id": "420ae0e9dc3bde52"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Mean Absolute Error (MAE)\n",
    "\n",
    "Для одного объекта:\n",
    "\n",
    "$$L(y, a) = |a - y|$$\n",
    "\n",
    "Для всей выборки:\n",
    "\n",
    "$$Q(w) = MAE(\\alpha, X) = \\frac{1}{l} \\sum_{i = 1}^l |\\alpha(x_i) - y_i| = \\frac{1}{l} \\sum_{i = 1}^l | \\langle w, x_i \\rangle - y_i |$$\n",
    "\n",
    "Проблема заключается в том, что функция $|x|$ является негладкой:\n",
    "\n",
    "- При $x > 0: f'(x) = 1$\n",
    "- При $x < 0: f'(x) = -1$\n",
    "- При $x = 0$ имеем излом $\\Rightarrow$ производной не существует\n",
    "\n",
    "Поэтому будем использовать субградиент!\n",
    "\n",
    "Субградиент функции $f$ в точке $x_0$ - это такой вектор $g$, что для любой точки $x$ выполняется неравенство:\n",
    "\n",
    "$$f(x) \\geq f(x_0) + \\langle g, x - x_0 \\rangle$$\n",
    "\n",
    "Если функция дифференцируема в точке $x_0$, то субградиент совпадает с градиентом.\n",
    "\n",
    "Давайте найдем субградиент для нашей функции $f(x)$ в точке $x_0 = 0$:\n",
    "\n",
    "$$f(x) \\geq f(0) + \\langle g, x - 0 \\rangle$$\n",
    "\n",
    "$$|x| \\geq 0 + g \\cdot x$$\n",
    "\n",
    "Рассмотрим два случая:\n",
    "\n",
    "1. $x > 0$: тогда $|x| = x$, следовательно:\n",
    "\n",
    "   $$x \\geq g \\cdot x \\Rightarrow g \\leq 1$$\n",
    "\n",
    "2. $x < 0$: тогда $|x| = -x$, следовательно:\n",
    "    $$-x \\geq g \\cdot x \\Rightarrow g \\geq -1$$\n",
    "\n",
    "По итогу получаем, что субградиент в точке $x_0 = 0$ может принимать любые значения из отрезка $[-1, 1]$.\n",
    "\n",
    "Как правило, выбирают $g = 0$.\n",
    "\n",
    "Теперь найдем субградиент функции потерь $Q(w)$:\n",
    "\n",
    "$$\\frac{\\partial Q}{\\partial w_j} = \\frac{\\partial}{\\partial w_j} \\left[ \\frac{1}{l} \\sum_{i = 1}^l | \\langle w, x_i \\rangle - y_i | \\right] = \\frac{1}{l} \\sum_{i = 1}^l \\frac{\\partial}{\\partial w_j} | \\langle w, x_i \\rangle - y_i | =$$\n",
    "\n",
    "$$\\frac{1}{l} \\sum_{i = 1}^l \\text{sign}(\\langle w, x_i \\rangle - y_i) \\cdot \\frac{\\partial}{\\partial w_j} (\\langle w, x_i \\rangle - y_i) = \\frac{1}{l} \\sum_{i = 1}^l \\text{sign} (\\langle w, x_i \\rangle - y_i) \\cdot x_i$$\n",
    "\n",
    "В матричной форме:\n",
    "\n",
    "$$\\nabla Q = \\frac{1}{l} X^T \\cdot \\text{sign} (Xw - y)$$\n",
    "\n",
    "Где $\\text{sign}$ применяется поэлементно к вектору.\n",
    "\n",
    "Любой локальный минимум функции потерь $Q(w)$ будет также и глобальным минимумом, тк функция выпуклая. Доказательство тривиально $:)$\n",
    "\n",
    "Реализации будут ниже!"
   ],
   "id": "4811e224f1440b37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Реализация градиентного спуска для MAE\n",
    "# TODO"
   ],
   "id": "5d16502d7710d51e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
